\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Machine Learning 2015: Project 3 - Feature Engineering}
\author{Ivaylo Toskov \\ itoskov@student.ethz.ch \and Maximilian Wurm \\ mwurm@student.ethz.ch \and 
	Martina Rivizzigno \\ rimartin@student.ethz.ch\\}
\date{\today}

\begin{document}
	\maketitle
	
	\section*{Experimental Protocol}
	This is the protocol of the group "I don't care".
	
	\section{Tools}
	In this project we splitted the work into a feature extraction and a classfication part.
	On the one hand we used Malab	for the image processing and feature extraction. 
	On the other hand, python together with numpy and the machine learning library "scikit-learn" was used for the subsequent classification.

	
	\section{Approach}
	Like mentioned above, the workflow for this project can be subdevided into two separate areas. 
	Firstly it is important to extract as many interesting features as possible from the underlying images. \\
	Only after that, one has enough information to classify the cells.

	
	\subsection{Feature Engineering}
	The area of computer vision offers several powerful methods and algorithms to extract different kinds of informations out of an image. The following ones have been taken into account within our reflections.

	\subsubsection{PHOG}
	\subsubsection{SIFT: Scale Invariant Feature Transform}
	This method describes local features in an image....
	
	\subsection{FCC: FREEMAN CHAIN CODE}
	The segmented images are first smooth out with an averaging filter to remove unncessary noise and threshold to a black and white image. Then the boundaries of the cell nucleus are retrieved as shown in Figure ~\ref{fig:boundary_good}. Subsapling of the boundary (Figure ~\ref{fig:subsampled_good}) is necessary because otherwise the chain code would not be representative of the general boundary. 
	
\begin{figure}[!tbp]
	\centering
	\subfloat[Boundary]{\includegraphics[width=0.35\textwidth]{boundary_good.png}\label{fig:boundary_good}}
	\qquad
	\subfloat[Subsampled boundary]{\includegraphics[width=0.35\textwidth]{subsampled_good.png}\label{fig:subsampled_good}}
	\caption{Cancer free cell nucleus}
\end{figure}
	
	\begin{figure}[!tbp]
		\centering
		\subfloat[Boundary]{\includegraphics[width=0.35\textwidth]{boundary_bad.png}\label{fig:boundary_bad}}
		\qquad
		\subfloat[Subsampled boundary]{\includegraphics[width=0.35\textwidth]{subsampled_bad.png}\label{fig:subsampled_bad}}
		\caption{Cancer cell nucleus} 
	\end{figure}
	
	Chain code are a way of encoding the boundary of an image. The code is composed of a sequence of numbers between 0 and 7. Each of them represents the transition between two consecutive pixels. Figure ~\ref{fig:fcc} explains the code assignment. For example a set up is encoded as a 0, a step digonally rigth/up as a 1 etc. The direction in which the boundary is sweept is not important but it has to be consistent for all the analysed images.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.23\textwidth]{fcc.png}
		\caption{Freeman chain code bits interpretation}
		\label{fig:fcc}
	\end{figure}
	
	As seen in Figure ~\ref{fig:boundary_good} and ~\ref{fig:boundary_bad}, the boundary of cancer free cell nucleus are much more similar to a circle than the cancer cells one. 


	\section{Classification}
	For the classification we used the basically same algorithm like in the last project. An ExtraTreeClassifier 
	Scikit-learn GridSearchCV function runs a cross validation based, fitting-performance test over all combinations of possible parameters, which are given in a separate list. The cross validation made it possible to immediately estimate the precision of a certain configuration. 
	
	\section{Lessons Learned} 
	In this project it became obvious that it is very important to extract proper features. Having proper features means not only to extract as many reasonable dimensions from the image as possible, but also to select the most interesting ones.

	Concerning classification, the same aspects as for the last project were valid: Tweaking a single classifier, at some points reached its limit, so that no further progress was achievable by tweaking the parameters and the ensemble ExtraTreeClassifier has been chosen again.
	
\end{document}
